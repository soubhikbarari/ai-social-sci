{
  "Ashwin, J., Chhabra, A., & Rao, V. (2023). Using large language models for qualitative analysis can introduce serious bias. arXiv preprint arXiv:2309.17147. \n- Publication: https://journals.sagepub.com/doi/10.1177/00491241251338246": {
    "title": "Using Large Language Models for Qualitative Analysis can Introduce Serious Bias",
    "url": "https://doi.org/10.1177/00491241251338246",
    "year": 2025,
    "authors": "Julian Ashwin, Aditya Chhabra, Vijayendra Rao",
    "journal": "Sociological Methods &amp; Research",
    "topics": [
      "Large Language Models",
      "Qualitative Data Analysis",
      "Bias in AI",
      "Supervised Learning",
      "Social Science Research"
    ],
    "takeaway": "Large language models (LLMs) can introduce bias when used to code and analyze qualitative data from open-ended interviews, leading to misleading inferences; training simpler supervised models on high-quality human codes leads to less measurement error and bias."
  },
  "Moore, J., Grabb, D., Agnew, W., Klyman, K., Chancellor, S., Ong, D. C., & Haber, N. (2025). Expressing stigma and inappropriate responses prevents LLMs from safely replacing mental health providers. In Proceedings of the 2025 ACM Conference on Fairness, Accountability, and Transparency (FAccT '25). Association for Computing Machinery.\n- Preprint: https://arxiv.org/abs/2504.18412\n- Github: https://github.com/jlcmoore/llms-as-therapists": {
    "title": "Expressing stigma and inappropriate responses prevents LLMs from safely replacing mental health providers",
    "url": "http://arxiv.org/abs/2504.18412v1",
    "year": "2025",
    "authors": "Jared Moore, Declan Grabb, William Agnew, Kevin Klyman, Stevie Chancellor, Desmond C. Ong, Nick Haber",
    "journal": "arXiv",
    "topics": [
      "Large Language Models",
      "Mental Health",
      "Therapeutic Relationships",
      "Tech Startups",
      "Clinical Therapy"
    ],
    "takeaway": "Large language models (LLMs) should not replace therapists due to their inability to adhere to crucial aspects of therapeutic relationships, their expression of stigma towards mental health conditions, and their inappropriate responses to certain conditions."
  },
  "von der Heyde, L., Haensch, A. C., & Wenz, A. (2024). Vox populi, vox ai? using language models to estimate german public opinion. arXiv preprint arXiv:2407.08563.\n- Preprint: https://arxiv.org/abs/2407.08563": {
    "title": "Vox Populi, Vox AI? Using Language Models to Estimate German Public Opinion",
    "url": "http://arxiv.org/abs/2407.08563v1",
    "year": "2024",
    "authors": "Leah von der Heyde, Anna-Carolina Haensch, Alexander Wenz",
    "journal": "arXiv",
    "topics": [
      "Large Language Models",
      "Public Opinion",
      "Synthetic Samples",
      "Vote Choice Prediction",
      "Opinion Representation"
    ],
    "takeaway": "Large language models, such as GPT-3.5, do not accurately predict citizens' vote choice in Germany, showing a bias towards the Green and Left parties and failing to capture the multifaceted factors influencing individual voter choices."
  },
  "United in Diversity? Contextual Biases in LLM-Based Predictions of the 2024 European Parliament Elections\nhttps://arxiv.org/abs/2409.09045": {
    "title": "United in Diversity? Contextual Biases in LLM-Based Predictions of the 2024 European Parliament Elections",
    "url": "http://arxiv.org/abs/2409.09045v2",
    "year": "2024",
    "authors": "Leah von der Heyde, Anna-Carolina Haensch, Alexander Wenz, Bolei Ma",
    "journal": "arXiv",
    "topics": [
      "Large Language Models",
      "Synthetic Samples",
      "Public Opinion Prediction",
      "Computational Social Science",
      "Bias in AI"
    ],
    "takeaway": "LLM-based predictions of future voting behavior largely fail, their accuracy is unequally distributed across national and linguistic contexts, and they require detailed attitudinal information in the prompt."
  },
  "AIn't Nothing But a Survey? Using Large Language Models for Coding German Open-Ended Survey Responses on Survey Motivation\nhttps://arxiv.org/abs/2506.14634": {
    "title": "AIn't Nothing But a Survey? Using Large Language Models for Coding German Open-Ended Survey Responses on Survey Motivation",
    "url": "http://arxiv.org/abs/2506.14634v3",
    "year": "2025",
    "authors": "Leah von der Heyde, Anna-Carolina Haensch, Bernd Wei\u00df, Jessica Daikeler",
    "journal": "arXiv",
    "topics": [
      "Language Learning Models",
      "Survey Research",
      "Open-ended Responses",
      "Automated Methods",
      "Predictive Performance"
    ],
    "takeaway": "The study found that the performance of different Language Learning Models (LLMs) in coding open-ended survey responses varies greatly, and only a fine-tuned LLM achieves satisfactory levels of predictive performance."
  },
  "Large Language Models Often Know When They Are Being Evaluated\nhttps://arxiv.org/abs/2505.23836": {
    "title": "Large Language Models Often Know When They Are Being Evaluated",
    "url": "http://arxiv.org/abs/2505.23836v3",
    "year": "2025",
    "authors": "Joe Needham, Giles Edkins, Govind Pimpale, Henning Bartsch, Marius Hobbhahn",
    "journal": "arXiv",
    "topics": [
      "AI models",
      "evaluation awareness",
      "real-world deployment",
      "agent trajectories",
      "model benchmarking"
    ],
    "takeaway": "Frontier AI models demonstrate above-random evaluation awareness and can identify the purpose of an evaluation, but do not yet surpass a simple human baseline."
  },
  "Mining Causality: AI-Assisted Search for Instrumental Variables\nhttps://arxiv.org/abs/2409.14202": {
    "title": "Mining Causality: AI-Assisted Search for Instrumental Variables",
    "url": "http://arxiv.org/abs/2409.14202v3",
    "year": "2024",
    "authors": "Sukjin Han",
    "journal": "arXiv",
    "topics": [
      "Instrumental Variables",
      "Causal Inference",
      "Large Language Models",
      "Economic Research",
      "Regression Analysis"
    ],
    "takeaway": "Large language models (LLMs) can be used to search for new instrumental variables (IVs) in economics, accelerating the process and exploring a larger search space than human researchers, as demonstrated with examples such as returns to schooling, supply and demand, and peer effects."
  },
  "Can AI Master Econometrics? Evidence from Econometrics AI Agent on Expert-Level Tasks\nhttps://arxiv.org/abs/2506.00856": {
    "title": "Can AI Master Econometrics? Evidence from Econometrics AI Agent on Expert-Level Tasks",
    "url": "http://arxiv.org/abs/2506.00856v2",
    "year": "2025",
    "authors": "Qiang Chen, Tianyang Han, Jin Li, Ye Luo, Yuxiao Wu, Xiaowei Zhang, Tuo Zhou",
    "journal": "arXiv",
    "topics": [
      "Artificial Intelligence",
      "Econometrics",
      "MetaGPT Framework",
      "Domain Expertise",
      "Coding Skills"
    ],
    "takeaway": "The 'Econometrics AI Agent' built on the open-source MetaGPT framework demonstrates superior performance in performing complex econometric tasks, outperforming large language models and general-purpose AI agents, and makes advanced econometric methods accessible to users with minimal coding skills."
  },
  "Can Large Language Models Learn Independent Causal Mechanisms?\nhttps://arxiv.org/abs/2402.02636": {
    "title": "Can Large Language Models Learn Independent Causal Mechanisms?",
    "url": "http://arxiv.org/abs/2402.02636v2",
    "year": "2024",
    "authors": "Ga\u00ebl Gendron, Bao Trung Nguyen, Alex Yuxuan Peng, Michael Witbrock, Gillian Dobbie",
    "journal": "arXiv",
    "topics": [
      "Large Language Models",
      "Causal Models",
      "Independent Causal Mechanisms",
      "Language Modelling",
      "Distribution Shifts"
    ],
    "takeaway": "Large Language Models' performance on uncommon settings or distribution shifts can be improved by applying concepts from causality to learn Independent Causal Mechanisms, leading to increased robustness and improved out-of-distribution performance on abstract and causal reasoning tasks."
  },
  "Exploring the Jungle of Bias: Political Bias Attribution in Language Models via Dependency Analysis\nhttps://arxiv.org/abs/2311.08605": {
    "title": "Exploring the Jungle of Bias: Political Bias Attribution in Language Models via Dependency Analysis",
    "url": "http://arxiv.org/abs/2311.08605v2",
    "year": "2023",
    "authors": "David F. Jenny, Yann Billeter, Mrinmaya Sachan, Bernhard Sch\u00f6lkopf, Zhijing Jin",
    "journal": "arXiv",
    "topics": [
      "Large Language Models",
      "Bias in AI",
      "Causal Fairness Analysis",
      "Activity Dependency Networks",
      "AI Alignment"
    ],
    "takeaway": "Bias in Large Language Models can be attributed to confounding and mitigating attributes and model misalignment, and can be analyzed through causal fairness analysis and Activity Dependency Networks."
  },
  "Promises and pitfalls of artificial intelligence for legal applications\nhttps://arxiv.org/abs/2402.01656": {
    "title": "Promises and pitfalls of artificial intelligence for legal applications",
    "url": "http://arxiv.org/abs/2402.01656v1",
    "year": "2024",
    "authors": "Sayash Kapoor, Peter Henderson, Arvind Narayanan",
    "journal": "arXiv",
    "topics": [
      "Artificial Intelligence",
      "Legal Profession",
      "Information Processing",
      "Creativity and Reasoning",
      "AI Evaluation"
    ],
    "takeaway": "While AI's role in the legal profession is growing, its ability to redefine the profession is overestimated, particularly in tasks involving creativity, reasoning, judgment, and future predictions."
  },
  "Mapping the Potential of Generative AI and Public Sector Work: Using time use data to identify opportunities for AI adoption in Great Britain's public sector\nhttps://www.turing.ac.uk/news/publications/mapping-potential-generative-ai-and-public-sector-work-using-time-use-data": {
    "title": "Exploring the Future of Public Sector Work: Generative AI Adoption amongst Dubai Government Employees Exploring the Future of Public Sector Work &lt;br&gt;",
    "url": "https://doi.org/10.2139/ssrn.5258102",
    "year": 2025,
    "authors": "Sarah Shaer, Keertana Subramani, Fadi Salem",
    "journal": "SSRN Electronic Journal",
    "topics": [
      "Public Sector Work",
      "Generative AI",
      "Government Employees",
      "Dubai",
      "AI Adoption"
    ],
    "takeaway": "The abstract discusses the adoption of Generative AI among Dubai Government Employees in the future of public sector work."
  },
  "Take caution in using LLMs as human surrogates\nhttps://www.pnas.org/doi/10.1073/pnas.2501660122": {
    "title": "Take caution in using LLMs as human surrogates",
    "url": "https://doi.org/10.1073/pnas.2501660122",
    "year": 2025,
    "authors": "Yuan Gao, Dokyun Lee, Gordon Burtch, Sina Fazelpour",
    "journal": "Proceedings of the National Academy of Sciences",
    "topics": [
      "Large Language Models",
      "Human Behavior Simulation",
      "Social Science Research",
      "Reasoning Depth",
      "11-20 Money Request Game"
    ],
    "takeaway": "Large Language Models (LLMs) fail to replicate human behavior distributions in the 11-20 money request game, suggesting caution in using LLMs as surrogates or for simulating human behavior in research."
  },
  "Predicting Empirical AI Research Outcomes with Language Models\nhttps://arxiv.org/abs/2506.00794": {
    "title": "Predicting Empirical AI Research Outcomes with Language Models",
    "url": "http://arxiv.org/abs/2506.00794v1",
    "year": "2025",
    "authors": "Jiaxin Wen, Chenglei Si, Yueh-han Chen, He He, Shi Feng",
    "journal": "arXiv",
    "topics": [
      "AI research",
      "Language Models",
      "Idea Generation",
      "Benchmarking",
      "Empirical Research"
    ],
    "takeaway": "A system combining a fine-tuned GPT-4.1 with a paper retrieval agent outperforms human experts and other language models in predicting the success of AI research ideas, demonstrating potential for accelerating empirical AI research."
  },
  "Generative Agent Simulations of 1,000 People\nhttps://arxiv.org/abs/2411.10109": {
    "title": "Generative Agent Simulations of 1,000 People",
    "url": "http://arxiv.org/abs/2411.10109v1",
    "year": "2024",
    "authors": "Joon Sung Park, Carolyn Q. Zou, Aaron Shaw, Benjamin Mako Hill, Carrie Cai, Meredith Ringel Morris, Robb Willer, Percy Liang, Michael S. Bernstein",
    "journal": "arXiv",
    "topics": [
      "human behavioral simulation",
      "computational agents",
      "policymaking",
      "social science",
      "agent architecture"
    ],
    "takeaway": "A novel agent architecture that simulates the attitudes and behaviors of real individuals has been developed, which can replicate participants' responses with high accuracy and reduces accuracy biases across racial and ideological groups."
  },
  "LLMs generate structurally realistic social networks but overestimate political homophily\nhttps://arxiv.org/abs/2408.16629": {
    "title": "LLMs generate structurally realistic social networks but overestimate political homophily",
    "url": "http://arxiv.org/abs/2408.16629v2",
    "year": "2024",
    "authors": "Serina Chang, Alicja Chaszczewicz, Emma Wang, Maya Josifovska, Emma Pierson, Jure Leskovec",
    "journal": "arXiv",
    "topics": [
      "Social Networks",
      "Generative AI",
      "Large Language Models",
      "Network Generation",
      "Bias in AI"
    ],
    "takeaway": "Large language models (LLMs) can generate realistic social networks using 'local' methods, but they tend to overemphasize political homophily compared to real social networks."
  },
  "ChatBench: From Static Benchmarks to Human-AI Evaluation\nhttps://arxiv.org/abs/2504.07114": {
    "title": "ChatBench: From Static Benchmarks to Human-AI Evaluation",
    "url": "http://arxiv.org/abs/2504.07114v1",
    "year": "2025",
    "authors": "Serina Chang, Ashton Anderson, Jake M. Hofman",
    "journal": "arXiv",
    "topics": [
      "LLM-based chatbots",
      "User-AI interaction",
      "Interactive evaluation",
      "User simulator",
      "ChatBench dataset"
    ],
    "takeaway": "The study finds that AI-alone accuracy does not predict user-AI accuracy, and fine-tuning a user simulator on a subset of ChatBench improves its ability to estimate user-AI accuracies."
  },
  "Language Model Fine-Tuning on Scaled Survey Data for Predicting Distributions of Public Opinions\nhttps://arxiv.org/abs/2502.16761": {
    "title": "Language Model Fine-Tuning on Scaled Survey Data for Predicting Distributions of Public Opinions",
    "url": "http://arxiv.org/abs/2502.16761v1",
    "year": "2025",
    "authors": "Joseph Suh, Erfan Jahanparast, Suhong Moon, Minwoo Kang, Serina Chang",
    "journal": "arXiv",
    "topics": [
      "Large Language Models",
      "Public Opinion Research",
      "Survey Design",
      "Predictive Modeling",
      "Data Fine-Tuning"
    ],
    "takeaway": "Fine-tuning large language models on a curated dataset of public opinion surveys greatly improves the match between model predictions and human responses, reducing the gap by up to 46% compared to baselines."
  },
  "Bias in Large Language Models: Origin, Evaluation, and Mitigation\nhttps://arxiv.org/abs/2411.10915": {
    "title": "Bias in Large Language Models: Origin, Evaluation, and Mitigation",
    "url": "http://arxiv.org/abs/2411.10915v1",
    "year": "2024",
    "authors": "Yufei Guo, Muzhe Guo, Juntao Su, Zhou Yang, Mengqiu Zhu, Hongfei Li, Mengyang Qiu, Shuo Shuo Liu",
    "journal": "arXiv",
    "topics": [
      "Large Language Models",
      "Bias in AI",
      "Bias Evaluation Methods",
      "Bias Mitigation Strategies",
      "Ethical and Legal Implications of AI"
    ],
    "takeaway": "This review provides a comprehensive examination of bias in Large Language Models (LLMs), including its origins, manifestations, evaluation methods, and mitigation strategies, as well as the ethical and legal implications of biased LLMs."
  },
  "Bias and Fairness in Large Language Models: A Survey\nhttps://arxiv.org/abs/2309.00770": {
    "title": "Bias and Fairness in Large Language Models: A Survey",
    "url": "http://arxiv.org/abs/2309.00770v3",
    "year": "2023",
    "authors": "Isabel O. Gallegos, Ryan A. Rossi, Joe Barrow, Md Mehrab Tanjim, Sungchul Kim, Franck Dernoncourt, Tong Yu, Ruiyi Zhang, Nesreen K. Ahmed",
    "journal": "arXiv",
    "topics": [
      "Large Language Models",
      "Bias Evaluation",
      "Bias Mitigation",
      "Natural Language Processing",
      "Social Bias"
    ],
    "takeaway": "The paper presents a comprehensive survey of bias evaluation and mitigation techniques for large language models (LLMs), proposing three taxonomies for bias evaluation and mitigation, and identifying open problems and challenges for future work."
  },
  "Measurement in the Age of LLMs: An Application to Ideological Scaling\nhttps://arxiv.org/abs/2312.09203": {
    "title": "Measurement in the Age of LLMs: An Application to Ideological Scaling",
    "url": "http://arxiv.org/abs/2312.09203v2",
    "year": "2023",
    "authors": "Sean O'Hagan, Aaron Schein",
    "journal": "arXiv",
    "topics": [
      "Social Science",
      "Large Language Models",
      "Political Ideology",
      "Text Analysis",
      "Measurement Tasks"
    ],
    "takeaway": "Large language models (LLMs) can be used to accurately and flexibly measure and characterize political ideology in text, according to established methods and judgement."
  },
  "Large Language Models Reflect the Ideology of their Creators\nhttps://arxiv.org/abs/2410.18417": {
    "title": "Large Language Models Reflect the Ideology of their Creators",
    "url": "http://arxiv.org/abs/2410.18417v2",
    "year": "2024",
    "authors": "Maarten Buyl, Alexander Rogiers, Sander Noels, Guillaume Bied, Iris Dominguez-Catena, Edith Heiter, Iman Johary, Alexandru-Cristian Mara, Rapha\u00ebl Romero, Jefrey Lijffijt, Tijl De Bie",
    "journal": "arXiv",
    "topics": [
      "Large Language Models",
      "Artificial Intelligence",
      "Political Bias",
      "Geopolitical Differences",
      "Regulatory Efforts"
    ],
    "takeaway": "The ideological stance of large language models appears to reflect the worldview of its creators, posing the risk of political instrumentalization and raising concerns about efforts to make these models ideologically 'unbiased'."
  },
  "Ferrara E (2024). The Butterfly Effect in artificial intelligence systems: Implications for AI bias and fairness. Machine Learning with Applications, Volume 15.\nhttps://www.sciencedirect.com/science/article/pii/S266682702400001X": {
    "title": "The Butterfly Effect in Artificial Intelligence Systems: Implications for AI Bias and Fairness",
    "url": "https://doi.org/10.2139/ssrn.4614234",
    "year": 2023,
    "authors": "Emilio Ferrara",
    "journal": "SSRN Electronic Journal",
    "topics": [
      "Artificial Intelligence",
      "AI Bias",
      "AI Fairness"
    ],
    "takeaway": "The abstract discusses the butterfly effect in artificial intelligence systems, focusing on its implications for AI bias and fairness."
  },
  "Potemkin Understanding in Large Language Models\nhttps://arxiv.org/abs/2506.21521": {
    "title": "Potemkin Understanding in Large Language Models",
    "url": "http://arxiv.org/abs/2506.21521v2",
    "year": "2025",
    "authors": "Marina Mancoridis, Bec Weeks, Keyon Vafa, Sendhil Mullainathan",
    "journal": "arXiv",
    "topics": [
      "Large Language Models",
      "Benchmark Datasets",
      "Concept Understanding",
      "Potemkin Understanding",
      "Concept Representations"
    ],
    "takeaway": "Large language models (LLMs) often demonstrate potemkin understanding, an illusion of comprehension that doesn't align with human concept interpretation, suggesting internal incoherence in concept representations."
  },
  "AI-augmented government transformation: Organisational transformation and the sociotechnical implications of artificial intelligence in public administrations\nhttps://www.sciencedirect.com/science/article/pii/S0740624X25000498": {
    "title": "AI-augmented government transformation: Organisational transformation and the sociotechnical implications of artificial intelligence in public administrations",
    "url": "https://doi.org/10.1016/j.giq.2025.102055",
    "year": 2025,
    "authors": "Luca Tangi, A. Paula Rodriguez M\u00fcller, Marijn Janssen",
    "journal": "Government Information Quarterly",
    "topics": [
      "Artificial Intelligence",
      "Public Administration",
      "Organizational Transformation",
      "Sociotechnical Systems",
      "Government Transformation"
    ],
    "takeaway": "The study explores the organizational transformation and sociotechnical implications of artificial intelligence in public administrations."
  },
  "Boelaert, J., Coavoux, S., Ollion, \u00c9., Petev, I., & Pr\u00e4g, P. (2025). Machine Bias. How Do Generative Language Models Answer Opinion Polls?. Sociological Methods & Research, 54(3), 1156-1196. https://doi.org/10.1177/00491241251330582 (Original work published 2025)": {
    "title": "Machine Bias. How Do Generative Language Models Answer Opinion Polls? <sup/>",
    "url": "https://doi.org/10.1177/00491241251330582",
    "year": 2025,
    "authors": "Julien Boelaert, Samuel Coavoux, \u00c9tienne Ollion, Ivaylo Petev, Patrick Pr\u00e4g",
    "journal": "Sociological Methods &amp; Research",
    "topics": [
      "Generative Artificial Intelligence",
      "Research Subjects",
      "Large Language Models",
      "Machine Bias",
      "Opinion or Attitudinal Research"
    ],
    "takeaway": "Generative AI models cannot replace human research subjects for opinion or attitudinal research due to their strong bias and low variance on each topic, a pattern labeled as 'machine bias'."
  },
  "Machine Bias. How Do Generative Language Models Answer Opinion Polls?\nhttps://journals.sagepub.com/doi/10.1177/00491241251330582": {
    "title": "Machine Bias. How Do Generative Language Models Answer Opinion Polls? <sup/>",
    "url": "https://doi.org/10.1177/00491241251330582",
    "year": 2025,
    "authors": "Julien Boelaert, Samuel Coavoux, \u00c9tienne Ollion, Ivaylo Petev, Patrick Pr\u00e4g",
    "journal": "Sociological Methods &amp; Research",
    "topics": [
      "Generative AI",
      "Research Subjects",
      "Opinion Research",
      "Machine Bias",
      "Large Language Models"
    ],
    "takeaway": "Generative AI models cannot replace human research subjects for opinion or attitudinal research due to their strong bias and low variance on each topic, a pattern labeled as 'machine bias'."
  },
  "A foundation model to predict and capture human cognition\nhttps://www.nature.com/articles/s41586-025-09215-4": {
    "title": "Attention capture is temporally stable: Evidence from mixed-model correlations",
    "url": "https://doi.org/10.1016/j.cognition.2018.07.013",
    "year": 2018,
    "authors": "Hanna Weichselbaum, Christoph Huber-Huber, Ulrich Ansorge",
    "journal": "Cognition",
    "topics": [
      "Attention Capture",
      "Temporal Stability",
      "Mixed-Model Correlations"
    ],
    "takeaway": "The study provides evidence that attention capture is temporally stable."
  },
  "Emergent analogical reasoning in large language models\nhttps://www.nature.com/articles/s41562-023-01659-w": {
    "title": "Evidence from counterfactual tasks supports emergent analogical reasoning in large language models",
    "url": "https://doi.org/10.1093/pnasnexus/pgaf135",
    "year": 2025,
    "authors": "Taylor W Webb, Keith J Holyoak, Hongjing Lu",
    "journal": "PNAS Nexus",
    "topics": [
      "Language Models",
      "Analogical Reasoning",
      "Zero-shot Performance",
      "Counterfactual Tasks",
      "Code Execution"
    ],
    "takeaway": "Language models, when augmented with the ability to write and execute code, are capable of generalizing to new counterfactual task variants, further supporting the emergence of a capacity for analogical reasoning in large language models."
  },
  "Large-scale AI language systems display an emergent ability to reason by analogy\nhttps://www.nature.com/articles/s41562-023-01671-0": {
    "title": "Large-scale AI language systems display an emergent ability to reason by analogy",
    "url": "https://doi.org/10.1038/s41562-023-01671-0",
    "year": 2023,
    "authors": "",
    "journal": "Nature Human Behaviour",
    "topics": [
      "AI",
      "Language Systems",
      "Reasoning",
      "Analogy",
      "Large-scale Systems"
    ],
    "takeaway": "Large-scale AI language systems have shown an emergent ability to reason by analogy."
  },
  "Could AI Leapfrog the Web? Evidence from Teachers in Sierra Leone\nhttps://arxiv.org/abs/2502.12397": {
    "title": "Could AI Leapfrog the Web? Evidence from Teachers in Sierra Leone",
    "url": "http://arxiv.org/abs/2502.12397v2",
    "year": "2025",
    "authors": "Daniel Bj\u00f6rkegren, Jun Ho Choi, Divya Budihal, Dominic Sobhani, Oliver Garrod, Paul Atherton",
    "journal": "arXiv",
    "topics": [
      "Artificial Intelligence",
      "Internet Usage",
      "Data Cost",
      "Web Search",
      "Low-Connectivity Regions"
    ],
    "takeaway": "AI-driven solutions can cost-effectively bridge information gaps in low-connectivity regions, as they are 87% less expensive than web search and are rated as more relevant, helpful, and correct by users."
  },
  "Your Brain on ChatGPT: Accumulation of Cognitive Debt when Using an AI Assistant for Essay Writing Task\n- https://arxiv.org/abs/2506.08872\n- https://www.brainonllm.com": {
    "title": "Generative AI and English essay writing: exploring the role of ChatGPT in enhancing learners\u2019 task engagement",
    "url": "https://doi.org/10.1093/applin/amaf045",
    "year": 2025,
    "authors": "Javad Zare, Fatemeh Ranjbaran Madiseh, Ali Derakhshan",
    "journal": "Applied Linguistics",
    "topics": [
      "Artificial Intelligence",
      "L2 Education",
      "Task Engagement",
      "Writing Tasks",
      "AI-based Tools"
    ],
    "takeaway": "The use of AI-based tools like ChatGPT significantly enhances learners' engagement in writing English essays, increasing motivation, reducing anxiety, and enhancing interest."
  },
  "A National Synthetic Populations Dataset for the United States\nhttps://www.nature.com/articles/s41597-025-04380-7": {
    "title": "HIV Prevalence Trends in Selected Populations in the United States: Results From National Serosurveillance, 1993-1997",
    "url": "https://doi.org/10.1037/e614462007-001",
    "year": 2001,
    "authors": "",
    "journal": "PsycEXTRA Dataset",
    "topics": [
      "HIV",
      "Prevalence",
      "United States",
      "Serosurveillance",
      "Trends"
    ],
    "takeaway": "The study presents the trends in HIV prevalence in selected populations in the United States from 1993 to 1997."
  },
  "The Mixed Subjects Design: Treating Large Language Models as Potentially Informative Observations\nhttps://journals.sagepub.com/doi/10.1177/00491241251326865": {
    "title": "The Mixed Subjects Design: Treating Large Language Models as Potentially Informative Observations",
    "url": "https://doi.org/10.1177/00491241251326865",
    "year": 2025,
    "authors": "David Broska, Michael Howes, Austin van Loon",
    "journal": "Sociological Methods &amp; Research",
    "topics": [
      "Large Language Models",
      "Human Behavior Prediction",
      "Mixed Subjects Design",
      "Prediction-Powered Inference",
      "Effective Sample Size"
    ],
    "takeaway": "Large language models can provide cost-effective but potentially inaccurate predictions of human behavior, and should be used in conjunction with human subjects to obtain valid estimates of causal effects and other parameters."
  },
  "The Levers of Political Persuasion with Conversational AI\nhttps://www.arxiv.org/abs/2507.13919": {
    "title": "The Levers of Political Persuasion with Conversational AI",
    "url": "http://arxiv.org/abs/2507.13919v1",
    "year": "2025",
    "authors": "Kobi Hackenburg, Ben M. Tappin, Luke Hewitt, Ed Saunders, Sid Black, Hause Lin, Catherine Fist, Helen Margetts, David G. Rand, Christopher Summerfield",
    "journal": "arXiv",
    "topics": [
      "Conversational AI",
      "Persuasion",
      "Post-training",
      "Prompting methods",
      "Factual accuracy"
    ],
    "takeaway": "The persuasive power of AI is likely to stem more from post-training and prompting methods than from personalization or increasing model scale, and these methods, while increasing persuasiveness, also systematically decrease factual accuracy."
  },
  "Market Research:\n- Synthetic Survey Data? It's Not Data https://quantuxblog.com/synthetic-survey-data-its-not-data\n- Synthetic respondents are the homoeopathy of market research https://conjointly.com/blog/synthetic-respondents-are-the-homeopathy-of-market-research/#appendix-1-code-for-a-demonstration-of-test-retest-unreliability-of-fake-respondents\n- Do LLMs simulate human attitudes about technology products? https://drive.google.com/file/d/16F_JZv4eHNiDMJT6BT7F6m97C2rBX8-7/view": {
    "title": "Do Survey Respondents and Non-respondents Differ? Ecological Analyses of the 2005 British Election Study",
    "url": "https://doi.org/10.1177/147078530604800304",
    "year": 2006,
    "authors": "Ron Johnston, Richard Harris",
    "journal": "International Journal of Market Research",
    "topics": [
      "Survey Analysis",
      "Election Study",
      "Ecological Analysis",
      "Respondent Behavior"
    ],
    "takeaway": "The study analyzes if there are differences between survey respondents and non-respondents in the context of the 2005 British Election Study."
  },
  "Policy Briefs & Reports:\n- Dispelling Myths of AI and Efficiency https://datasociety.net/wp-content/uploads/2025/03/Dispelling-Myths-of-AI-and-Efficiency-1.pdf\n- AI-Ready Federal Statistical Data: An Extension of Communicating Data Quality https://www.fcsm.gov/assets/files/docs/FCSM.25.03_AI-Ready-Extension-Data-Quality.pdf\n- The Impact of Generative AI on Work Productivity\n https://www.stlouisfed.org/on-the-economy/2025/feb/impact-generative-ai-work-productivity#:~:text=Together%2C%20the%20model%20and%20data,that%20they%20use%20generative%20AI.": {
    "title": "Investigating the productivity impact of Generative AI in the public sector",
    "url": "https://doi.org/10.1257/rct.14140",
    "year": 2024,
    "authors": "Christopher Wong, Andrew Leigh",
    "journal": "AEA Randomized Controlled Trials",
    "topics": [
      "Generative AI",
      "Productivity",
      "Public Sector"
    ],
    "takeaway": "The study investigates the impact of Generative AI on productivity in the public sector."
  }
}